## ğŸ“… Week 1, Day 1 â€“ **Cloud AI/ML Setup (AWS Focus)**

### ğŸ§  Goal:

By the end of today, you'll have:

- âœ… Set up AWS CLI + authenticated
- âœ… Provisioned an **S3 bucket** for ML data
- âœ… Launched a **SageMaker Studio notebook**
- âœ… Connected to it via browser
- âœ… Loaded the **Titanic dataset** into S3
- âœ… Run a **cloud-hosted Jupyter notebook** for basic EDA
- âœ… Set up Terraform to do this automatically in the future
- âœ… Committed progress to GitHub & Kanban

---

## ğŸš€ Step 1 â€“ AWS CLI Auth + Environment Setup

### âœ… Check AWS CLI

```bash
aws --version
```

### âœ… Configure Credentials

```bash
aws configure
```

You'll need:

- **Access Key ID**
- **Secret Access Key**
- **Default region** (use `us-east-1` unless you prefer another)
- **Output format** â†’ `json`

Test:

```bash
aws sts get-caller-identity
```

---

## ğŸ“¦ Step 2 â€“ Create an S3 Bucket for ML Data

```bash
aws s3 mb s3://ml-titanic-w1d1-wthurston --region us-east-1
```

ğŸ“ Upload Titanic CSV (you can get it from [Kaggle](https://www.kaggle.com/c/titanic/data) or use a cleaned version I can provide):

```bash
aws s3 cp titanic.csv s3://ml-titanic-w1d1-wthurston/data/
```

---

## ğŸ““ Step 3 â€“ Launch SageMaker Studio

> Weâ€™ll use **SageMaker Studio** (not SageMaker Notebook Instances â€” itâ€™s the modern way)

### ğŸ› ï¸ A. Create an execution role

```bash
aws iam create-role \
  --role-name sagemaker-execution-role \
  --assume-role-policy-document file://shared/terraform/iam-trust-policy.json
```

> I can generate this policy file if you like

### ğŸ› ï¸ B. Launch Studio via Console

(SageMaker Studio is easiest to launch via AWS Console, but you can automate it later with Terraform.)

1. Go to **SageMaker â†’ Studio â†’ Domain**
2. Choose **â€œQuick setupâ€**
3. Pick your **IAM role** and region
4. Wait 5â€“10 mins while it boots up

---

## ğŸ“Š Step 4 â€“ Run Your First Cloud Notebook

Inside SageMaker Studio:

```python
# s3_test_load.ipynb

import boto3
import pandas as pd
from io import StringIO

s3 = boto3.client('s3')
bucket = 'ml-titanic-w1d1-wthurston'
obj = s3.get_object(Bucket=bucket, Key='data/titanic.csv')

df = pd.read_csv(obj['Body'])
df.head()
```

Thatâ€™s it â€” your notebook is now **running on AWS**, using cloud storage!

---

## ğŸ§± Step 5 â€“ (Optional) Terraform Starter for S3 Bucket

If you want to try deploying the S3 bucket with Terraform:

```hcl
# shared/terraform/aws-s3-bucket.tf
resource "aws_s3_bucket" "ml_bucket" {
  bucket = "ml-titanic-w1d1-wthurston"
  force_destroy = true
}
```

Then:

```bash
cd shared/terraform
terraform init
terraform plan
terraform apply
```

---

## ğŸ§ª Exercises for Today

1. âœ… Launch SageMaker Studio in `us-east-1`
2. âœ… Upload Titanic dataset to S3 (`aws s3 cp`)
3. âœ… Load S3 file in SageMaker notebook using `boto3`
4. âœ… Commit notebook to `aws/ingestion/`
5. âœ… Run `pre-commit run --all-files`
6. âœ… Mark Kanban card "Run basic EDA notebook" as âœ… Done

---
