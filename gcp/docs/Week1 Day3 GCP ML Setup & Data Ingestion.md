**Letâ€™s go!** ðŸš€
Welcome to **Week 1, Day 3**: today is all about **Google Cloud Platform (GCP)** and getting hands-on with:

- âœ… Google Cloud Storage (GCS)
- âœ… Vertex AI Workbench (JupyterLab)
- âœ… Python SDK for data ingestion
- âœ… GCP IAM permissions
- âœ… Dataset upload, notebook access, cleanup

---

## ðŸ“… Week 1, Day 3 â€“ GCP ML Setup & Data Ingestion

### ðŸ§  Goal:

By the end of today, you will:

- Upload Titanic dataset to **Google Cloud Storage**
- Set up a **Vertex AI Workbench instance**
- Launch a notebook and load the dataset
- Commit your notebook to GitHub
- Clean up all GCP resources

---

## ðŸ› ï¸ Step 1: Check GCP CLI & Auth

### âœ… Verify `gcloud` CLI:

```bash
gcloud version
```

### âœ… Log In:

```bash
gcloud auth login
```

### âœ… Set Project (if needed):

```bash
gcloud config set project YOUR_PROJECT_ID
```

---

## ðŸ“¦ Step 2: Create a GCS Bucket + Upload Titanic Dataset

### âœ… Create Bucket:

```bash
gsutil mb -p YOUR_PROJECT_ID -l us-central1 gs://ml-titanic-w1d3-yourname/
```

### âœ… Upload Titanic Dataset:

```bash
gsutil cp shared/datasets/titanic.csv gs://ml-titanic-w1d3-yourname/data/
```

---

### ðŸ§­ **Create a Managed Workbench Notebook**

1. Go to: [Vertex AI â†’ Workbench](https://console.cloud.google.com/vertex-ai/workbench)
2. Click: **â€œNew Notebook â†’ Managed notebooksâ€**
3. Choose:
   - **Region**: `us-central1`
   - **Environment**: `Python (3.10)` or latest
   - **Machine type**: `e2-standard-2` or similar
   - Auto-pause: `Idle for 20 minutes`
4. Click **Create**

Then click **â€œOpen JupyterLabâ€** when itâ€™s ready.

---

## ðŸ“” Step 4: Load Titanic Data in Python

Inside your Vertex AI JupyterLab notebook:

```python
from google.cloud import storage
import pandas as pd

client = storage.Client()
bucket = client.bucket('ml-titanic-w1d3-yourname')
blob = bucket.blob('data/titanic.csv')

content = blob.download_as_text()
df = pd.read_csv(pd.compat.StringIO(content))
df.head()
```

> ðŸ’¡ If this throws an error, try enabling the API or installing the SDK:
> `!pip install google-cloud-storage`

---

## ðŸ§ª Day 3 Exercises

1. âœ… Create GCS bucket
2. âœ… Upload Titanic dataset
3. âœ… Launch Vertex notebook
4. âœ… Load dataset using `google-cloud-storage`
5. âœ… Save notebook as `gcp/ingestion/load_titanic_gcs.ipynb`
6. âœ… Commit + push to GitHub
7. âœ… Clean up GCP resources

---

## ðŸ”¥ Cleanup (to avoid charges)

```bash
gsutil rm -r gs://ml-titanic-w1d3-yourname
gcloud notebooks instances delete w1d3-notebook --location=us-central1
```

---
